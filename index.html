<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>QAM's Learning Materials</title>
</head>
<body>
    <h1>Machine Learning</h1>

    <!-- First Block of List -->
    <div>
        <h2>LLM / Prompt</h2>
        <ul>
            <li><a href="https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE" target="_blank">Ilya suggested papers</a></li>
            <li><a href="https://arxiv.org/abs/2409.10173" target="_blank">jina-embeddings-v3: Multilingual Embeddings With Task LoRA</a></li>
            <li><a href="https://www.youtube.com/watch?v=quh7z1q7-uc" target="_blank">(Youtube) Building LLMs from the Ground Up: A 3-hour Coding Workshop</a></li>
            <li><a href="https://arxiv.org/abs/2409.02060" target="_blank">OLMoE: Open Mixture-of-Experts Language Models</a></li>
            <li><a href="https://arxiv.org/abs/2205.13147" target="_blank">Matryoshka Representation Learning</a></li>
            <li><a href="https://arxiv.org/abs/2407.20798" target="_blank">Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning</a></li>
            <li><a href="https://arxiv.org/abs/2407.12994" target="_blank">A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks</a></li>
            <li><a href="https://arxiv.org/abs/2404.06910" target="_blank">Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation</a></li>
            <li><a href="https://arxiv.org/abs/2408.14906" target="_blank">Writing in the Margins: Better Inference Pattern for Long Context Retrieval</a></li>
            <li><a href="https://arxiv.org/abs/2408.16357" target="_blank">Law of Vision Representation in MLLMs</a></li>
            <li><a href="https://arxiv.org/abs/2406.17262" target="_blank">D2LLM: Decomposed and Distilled Large Language Models for Semantic Search</a></li>
            <li><a href="https://arxiv.org/abs/2403.02078" target="_blank">Automated Generation of Multiple-Choice Cloze Questions for Assessing English Vocabulary Using GPT-turbo 3.5</a></li>
            <li><a href="https://arxiv.org/abs/2411.03350" target="_blank">A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness</a></li>
            <li><a href="https://arxiv.org/abs/2411.13676" target="_blank">Hymba: A Hybrid-head Architecture for Small Language Models</a></li>
            <li><a href="https://github.com/huggingface/smollm" target="_blank">SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters</a></li>
            <li><a href="https://allenai.org/papers/tulu-3-report.pdf" target="_blank">TÃœLU 3: Pushing Frontiers in Open Language Model Post-Training</a></li>
            <li><a href="https://arxiv.org/abs/2412.03220v1" target="_blank">Survey of different Large Language Model Architectures: Trends, Benchmarks, and Challenges</a></li>
        </ul>
    </div>

    <!-- Block of List -->
    <div>
        <h2>LLM Acceleration</h2>
        <ul>
            <li><a href="https://arxiv.org/abs/2211.17192" target="_blank">Fast Inference from Transformers via Speculative Decoding</a></li>
            <li><a href="https://arxiv.org/abs/2302.01318" target="_blank">Accelerating Large Language Model Decoding with Speculative Sampling</a></li>
            <li><a href="https://arxiv.org/abs/2305.09781" target="_blank">SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification</a></li>
            <li><a href="https://arxiv.org/abs/2304.04487" target="_blank">Inference with Reference: Lossless Acceleration of Large Language Models</a></li>
            
        </ul>
    </div>
    
    <!-- Second Block of List -->
    <div>
        <h2>Retrieval-Augmented Generation</h2>
        <ul>
            <li><a href="https://arxiv.org/abs/2310.08560" target="_blank">MemGPT: Towards LLMs as Operating Systems</a></li>
            <li><a href="https://arxiv.org/abs/2409.10102" target="_blank">Trustworthiness in Retrieval-Augmented Generation Systems: A Survey</a></li>
            <li><a href="https://arxiv.org/abs/2409.10038" target="_blank">On the Diagram of Thought</a></li>
            <li><a href="https://arxiv.org/abs/2210.03629" target="_blank">ReAct: Synergizing Reasoning and Acting in Language Models</a></li>
            <li><a href="https://arxiv.org/abs/2402.14207" target="_blank">Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</a></li>
            <li><a href="https://arxiv.org/abs/2407.16833" target="_blank">Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach</a></li>
            <li><a href="https://arxiv.org/abs/2407.01219" target="_blank">Searching for Best Practices in Retrieval-Augmented Generation</a></li>
            <li><a href="https://arxiv.org/abs/2406.17419" target="_blank">Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA</a></li>
            <li><a href="https://arxiv.org/abs/2406.12566" target="_blank">RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation</a></li>
            <li><a href="https://arxiv.org/abs/2310.01558" target="_blank">Making Retrieval-Augmented Language Models Robust to Irrelevant Context</a></li>
            <li><a href="https://arxiv.org/abs/2405.20978" target="_blank">Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training</a></li>
            <li><a href="https://arxiv.org/abs/2411.02959v1" target="_blank">HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems</a></li>
        </ul>
    </div>

    <!-- Block of List -->
    <div>
        <h2>Embedding</h2>
        <ul>
            <li><a href="https://arxiv.org/abs/2004.12832" target="_blank">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</a></li>
            <li><a href="https://arxiv.org/abs/2409.04701" target="_blank">Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models</a></li>
            <li><a href="https://github.com/NVIDIA/Cosmos-Tokenizer?tab=readme-ov-file" target="_blank">Cosmos Tokenizer: A suite of image and video neural tokenizers</a></li>
            
        </ul>
    </div>
    
    <!-- Block of List -->
    <div>
        <h2>Agent</h2>
        <ul>
            <li><a href="https://arxiv.org/abs/2410.08328v1" target="_blank">Agents Thinking Fast and Slow: A Talker-Reasoner Architecture</a></li>
        </ul>
    </div>

    <!-- Third Block of List -->
    <div>
        <h2>Optimize / Fine tune model</h2>
        <ul>
            <li><a href="https://arxiv.org/abs/2407.14679" target="_blank">Compact Language Models via Pruning and Knowledge Distillation</a></li>
            <li><a href="https://arxiv.org/abs/2407.16216" target="_blank">A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More</a></li>
            <li><a href="https://arxiv.org/abs/2405.12130" target="_blank">MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning</a></li>
            <li><a href="https://arxiv.org/abs/2304.11277" target="_blank">PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel</a></li>
            <li><a href="https://research.character.ai/optimizing-inference/" target="_blank">Optimizing AI Inference at Character.AI</a></li>
        </ul>
    </div>

    <!-- Fourth Block of List -->
    <div>
        <h2>VLM / MLLM / Reasoning</h2>
        <ul>
            <li><a href="https://arxiv.org/abs/2409.14993v1" target="_blank">Multi-Modal Generative AI: Multi-modal LLM, Diffusion and Beyond</a></li>
            <li><a href="https://arxiv.org/abs/2406.09246" target="_blank">OpenVLA: An Open-Source Vision-Language-Action Model</a></li>
            <li><a href="https://arxiv.org/abs/2411.14405" target="_blank">Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions</a></li>
            <li><a href="https://arxiv.org/abs/2411.16489" target="_blank">O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?</a></li>
        </ul>
    </div>

    <!-- Fifth Block of List -->
    <div>
        <h2>Other</h2>
        <ul>
            <li><a href="https://arxiv.org/abs/2408.14837" target="_blank">Diffusion Models Are Real-Time Game Engines</a></li>
            <li><a href="https://arxiv.org/abs/2404.19756" target="_blank">KAN: Kolmogorov-Arnold Networks</a></li>
        </ul>
    </div>

    <!-- Sixth Block of List -->
    <div>
        <h2>RAG / Document handling library</h2>
        <ul>
            <li><a href="https://github.com/DS4SD/docling?tab=readme-ov-file" target="_blank">Docling parses documents and exports them to the desired format with ease and speed</a></li>
            <li><a href="https://github.com/crewAIInc/crewAI" target="_blank">CrewAI Agent framework</a></li>
        </ul>
    </div>
</body>
</html>
